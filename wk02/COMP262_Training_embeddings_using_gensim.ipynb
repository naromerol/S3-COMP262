{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WGPwjhbbwPT"
   },
   "source": [
    "## Training Embeddings Using Gensim\n",
    "Word embeddings are an approach to representing text in NLP. In this notebook we will demonstrate how to train embeddings using Genism. [Gensim](https://radimrehurek.com/gensim/index.html) is an open source Python library for natural language processing, with a focus on topic modeling (explained in chapter 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:40.863650Z",
     "start_time": "2021-04-05T21:26:40.339123Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TBw9OCYcYQ_n"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:40.894143Z",
     "start_time": "2021-04-05T21:26:40.865114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5qWptd54ZcfV"
   },
   "outputs": [],
   "source": [
    "# define training data\n",
    "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
    "#Every list contains lists of tokens of that document.\n",
    "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
    "\n",
    "#Training the model\n",
    "model_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\n",
    "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QjSxefPl4mh"
   },
   "source": [
    "## Continuous Bag of Words (CBOW) \n",
    "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:56.724662Z",
     "start_time": "2021-04-05T21:26:56.712651Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "nyZY8ME4lUjd",
    "outputId": "bd00e825-c11a-4b36-dbf5-80f32c659956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n",
      "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "  7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      " -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "  6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "  2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      " -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      " -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      " -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      " -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "  9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      " -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "  3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "  7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      " -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "  2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "  2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.index_to_key)\n",
    "print(words)\n",
    "\n",
    "#Access vector for one word\n",
    "print(model_cbow.wv['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:57.420196Z",
     "start_time": "2021-04-05T21:26:57.417193Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gMuHv52GeuoR",
    "outputId": "b498032d-6f9d-485b-a3cc-5a21300bfb06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between eats and bites: -0.013497107\n",
      "Similarity between eats and man: -0.052354384\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between eats and bites:\",model_cbow.wv.similarity('eats', 'bites'))\n",
    "print(\"Similarity between eats and man:\",model_cbow.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twhTZfPOezTU"
   },
   "source": [
    "From the above similarity scores we can conclude that eats is more similar to bites than man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:59.635831Z",
     "start_time": "2021-04-05T21:26:59.621818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5Lv0V7WofmsB",
    "outputId": "00600b23-d9a6-4f14-bacd-395be85076c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.13887983560562134),\n",
       " ('bites', 0.13149003684520721),\n",
       " ('eats', 0.06422409415245056),\n",
       " ('dog', 0.009391188621520996),\n",
       " ('man', -0.05987628176808357)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most similarity\n",
    "model_cbow.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:59.855822Z",
     "start_time": "2021-04-05T21:26:59.841810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WA783nrSalgs",
    "outputId": "80d6e23f-2bed-47d7-f925-4aaa87ec5f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_cbow.save('model_cbow.bin')\n",
    "\n",
    "# load model\n",
    "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
    "print(new_model_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deReLSI7mQyr"
   },
   "source": [
    "## SkipGram\n",
    "In skipgram, the task is to predict the context words from the center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:00.517046Z",
     "start_time": "2021-04-05T21:27:00.508038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "9QtUtsLglvY0",
    "outputId": "6d19902b-66aa-4b0f-9f12-be18f37d40d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n",
      "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "  7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      " -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "  6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "  2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      " -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      " -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      " -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      " -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "  9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      " -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "  3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "  7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      " -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "  2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "  2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_skipgram)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_skipgram.wv.index_to_key)\n",
    "print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(model_skipgram.wv['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:02.660747Z",
     "start_time": "2021-04-05T21:27:02.642866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8YUsblEOfFWf",
    "outputId": "14cd759c-d5fc-465f-ed20-8fd1a1949168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between eats and bites: -0.01351881\n",
      "Similarity between eats and man: -0.052345112\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between eats and bites:\",model_skipgram.wv.similarity('eats', 'bites'))\n",
    "print(\"Similarity between eats and man:\",model_skipgram.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdXVDePKnBpv"
   },
   "source": [
    "From the above similarity scores we can conclude that eats is more similar to bites than man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:03.419546Z",
     "start_time": "2021-04-05T21:27:03.414541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "lpF4qtwpmuM3",
    "outputId": "f3bc68f6-3768-4a4d-e5bc-bb3dff6f654f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.13887986540794373),\n",
       " ('bites', 0.1314900517463684),\n",
       " ('eats', 0.06406083703041077),\n",
       " ('dog', 0.00939119327813387),\n",
       " ('man', -0.05987628549337387)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most similarity\n",
    "model_skipgram.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:03.973454Z",
     "start_time": "2021-04-05T21:27:03.950433Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aNDCEXRTnAnj",
    "outputId": "402f77b6-0625-4b37-e135-3650df626007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_skipgram.save('model_skipgram.bin')\n",
    "\n",
    "# load model\n",
    "new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
    "print(model_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0MiqJ_1M0mX"
   },
   "source": [
    "## Training Your Embedding on Wiki Corpus\n",
    "\n",
    "##### The corpus download page : https://dumps.wikimedia.org/enwiki/20200120/\n",
    "The entire wiki corpus as of 28/04/2020 is just over 16GB in size.\n",
    "We will take a part of this corpus due to computation constraints and train our word2vec and fasttext embeddings.\n",
    "\n",
    "The file size is 294MB so it can take a while to download.\n",
    "\n",
    "Source for code which downloads files from Google Drive: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:58.596845Z",
     "start_time": "2021-04-05T21:27:58.585833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, skipping download\n",
      "File at: data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs('data/en', exist_ok= True)\n",
    "file_name = \"data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\"\n",
    "file_id = \"11804g0GcWnBIVDahjo5fQyc05nQLXGwF\"\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    download_file_from_google_drive(file_id, file_name)\n",
    "else:\n",
    "    print(\"file already exists, skipping download\")\n",
    "\n",
    "print(f\"File at: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T08:59:17.024306Z",
     "start_time": "2021-04-03T08:59:17.022304Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wX1kx96JLYvt"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T09:56:14.722195Z",
     "start_time": "2021-04-03T09:56:14.705177Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rJgsEUmRPppc"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Invalid data stream",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Preparing the Training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# wiki = WikiCorpus(file_name, lemmatize=False, dictionary={})\u001b[39;00m\n\u001b[0;32m      3\u001b[0m wiki \u001b[38;5;241m=\u001b[39m WikiCorpus(file_name, processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dictionary\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m----> 4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwiki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py:685\u001b[0m, in \u001b[0;36mWikiCorpus.get_texts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses, init_to_ignore_interrupt)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# process the corpus in smaller chunks of docs, because multiprocessing.Pool\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;66;03m# is dumb and would load the entire input into RAM at once...\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mchunkize(texts, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tokens, title, pageid \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap(_process_article, group):\n\u001b[0;32m    687\u001b[0m             articles_all \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\utils.py:1333\u001b[0m, in \u001b[0;36mchunkize\u001b[1;34m(corpus, chunksize, maxsize, as_numpy)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSX with python3.8+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m; aliasing chunkize to chunkize_serial\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m entity)\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunkize_serial(corpus, chunksize, as_numpy\u001b[38;5;241m=\u001b[39mas_numpy):\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\utils.py:1243\u001b[0m, in \u001b[0;36mchunkize_serial\u001b[1;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[0;32m   1241\u001b[0m     wrapped_chunk \u001b[38;5;241m=\u001b[39m [[np\u001b[38;5;241m.\u001b[39marray(doc, dtype\u001b[38;5;241m=\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(it, \u001b[38;5;28mint\u001b[39m(chunksize))]]\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     wrapped_chunk \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wrapped_chunk[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py:675\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    672\u001b[0m positions, positions_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    674\u001b[0m tokenization_params \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_min_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_max_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower)\n\u001b[1;32m--> 675\u001b[0m texts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    676\u001b[0m     (text, title, pageid, tokenization_params)\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m title, text, pageid\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m extract_pages(bz2\u001b[38;5;241m.\u001b[39mBZ2File(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_namespaces, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_articles)\n\u001b[0;32m    679\u001b[0m )\n\u001b[0;32m    680\u001b[0m pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses, init_to_ignore_interrupt)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# process the corpus in smaller chunks of docs, because multiprocessing.Pool\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;66;03m# is dumb and would load the entire input into RAM at once...\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py:412\u001b[0m, in \u001b[0;36mextract_pages\u001b[1;34m(f, filter_namespaces, filter_articles)\u001b[0m\n\u001b[0;32m    406\u001b[0m elems \u001b[38;5;241m=\u001b[39m (elem \u001b[38;5;28;01mfor\u001b[39;00m _, elem \u001b[38;5;129;01min\u001b[39;00m iterparse(f, events\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,)))\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We can't rely on the namespace for database dumps, since it's changed\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# it every time a small modification to the format is made. So, determine\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# those from the first element we find, which will be part of the metadata,\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# and construct element paths.\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m namespace \u001b[38;5;241m=\u001b[39m get_namespace(elem\u001b[38;5;241m.\u001b[39mtag)\n\u001b[0;32m    414\u001b[0m ns_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m\"\u001b[39m: namespace}\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py:406\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_pages\u001b[39m(f, filter_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, filter_articles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;124;03m\"\"\"Extract pages from a MediaWiki database dump.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     elems \u001b[38;5;241m=\u001b[39m (elem \u001b[38;5;28;01mfor\u001b[39;00m _, elem \u001b[38;5;129;01min\u001b[39;00m iterparse(f, events\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,)))\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# We can't rely on the namespace for database dumps, since it's changed\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# it every time a small modification to the format is made. So, determine\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# those from the first element we find, which will be part of the metadata,\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# and construct element paths.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(elems)\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\xml\\etree\\ElementTree.py:1229\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1229\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\bz2.py:182\u001b[0m, in \u001b[0;36mBZ2File.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_can_read()\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mC:\\Dev\\anaconda3\\envs\\comp262_2\\lib\\_compression.py:103\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m         rawblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Invalid data stream"
     ]
    }
   ],
   "source": [
    "#Preparing the Training data\n",
    "# wiki = WikiCorpus(file_name, lemmatize=False, dictionary={})\n",
    "wiki = WikiCorpus(file_name, processes=4, dictionary={})\n",
    "sentences = list(wiki.get_texts())\n",
    "\n",
    "#if you get a memory error executing the lines above\n",
    "#comment the lines out and uncomment the lines below. \n",
    "#loading will be slower, but stable.\n",
    "# wiki = WikiCorpus(file_name, processes=4, lemmatize=False, dictionary={})\n",
    "# sentences = list(wiki.get_texts())\n",
    "\n",
    "#if you still get a memory error, try settings processes to 1 or 2 and then run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsIrgt_gPQda"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "\n",
    "1.   sg - Selecting the training algorithm: 1 for skip-gram else its 0 for CBOW. Default is CBOW.\n",
    "2.   min_count-  Ignores all words with total frequency lower than this.<br>\n",
    "There are many more hyperparamaeters whose list can be found in the official documentation [here.](https://radimrehurek.com/gensim/models/word2vec.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:01:20.065332Z",
     "start_time": "2021-04-03T09:59:12.350872Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "idmfbr_8LvoN",
    "outputId": "f505a46e-025d-4169-f996-06c672008f81"
   },
   "outputs": [],
   "source": [
    "#CBOW\n",
    "start = time.time()\n",
    "word2vec_cbow = Word2Vec(sentences,min_count=10, sg=0)\n",
    "end = time.time()\n",
    "\n",
    "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:02:10.613551Z",
     "start_time": "2021-04-03T10:02:10.585535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "mMdGn08-RkhM",
    "outputId": "efb34148-3fb4-435c-f070-8493708fc07a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_cbow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Summarize the loaded model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mword2vec_cbow\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Summarize vocabulary\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec_cbow' is not defined"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(word2vec_cbow)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(word2vec_cbow.wv.vocab)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(word2vec_cbow['film'])}\")\n",
    "print(word2vec_cbow['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",word2vec_cbow.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",word2vec_cbow.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:02:16.109851Z",
     "start_time": "2021-04-03T10:02:15.257052Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rXrDOrKskcHX"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "from gensim.models import Word2Vec, KeyedVectors   \n",
    "word2vec_cbow.wv.save_word2vec_format('word2vec_cbow.bin', binary=True)\n",
    "\n",
    "# load model\n",
    "# new_modelword2vec_cbow = Word2Vec.load('word2vec_cbow.bin')\n",
    "# print(word2vec_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:08:27.736688Z",
     "start_time": "2021-04-03T10:02:19.197708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dX0U0CbQOK30",
    "outputId": "b9bfcf2b-91cb-40d9-ca92-791ec346aef4"
   },
   "outputs": [],
   "source": [
    "#SkipGram\n",
    "start = time.time()\n",
    "word2vec_skipgram = Word2Vec(sentences,min_count=10, sg=1)\n",
    "end = time.time()\n",
    "\n",
    "print(\"SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:09:06.406929Z",
     "start_time": "2021-04-03T10:09:06.383908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "LXnY9YInSvnI",
    "outputId": "26f1dab7-27a6-4655-81c7-ac6f08fe1f9c"
   },
   "outputs": [],
   "source": [
    "#Summarize the loaded model\n",
    "print(word2vec_skipgram)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(word2vec_skipgram.wv.vocab)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(word2vec_skipgram['film'])}\")\n",
    "print(word2vec_skipgram['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",word2vec_skipgram.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",word2vec_skipgram.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:09:09.947695Z",
     "start_time": "2021-04-03T10:09:09.076901Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o8U7bfPSVB04"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec_cbow.wv.save_word2vec_format('word2vec_sg.bin', binary=True)\n",
    "\n",
    "# load model\n",
    "# new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
    "# print(model_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kExlA8kfrKml"
   },
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:16:31.271764Z",
     "start_time": "2021-04-03T10:09:16.592670Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JPd2VhMEk8gL",
    "outputId": "55c44bdd-d7d8-4df2-8140-cdd442bbd68c"
   },
   "outputs": [],
   "source": [
    "#CBOW\n",
    "start = time.time()\n",
    "fasttext_cbow = FastText(sentences, sg=0, min_count=10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"FastText CBOW Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:16:31.287283Z",
     "start_time": "2021-04-03T10:16:31.273765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "FlQFl8-Zsost",
    "outputId": "6472e944-e6de-4d64-8c6f-14475ef1eac5"
   },
   "outputs": [],
   "source": [
    "#Summarize the loaded model\n",
    "print(fasttext_cbow)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(fasttext_cbow.wv.vocab)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(fasttext_cbow['film'])}\")\n",
    "print(fasttext_cbow['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",fasttext_cbow.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",fasttext_cbow.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:28:28.771383Z",
     "start_time": "2021-04-03T10:16:31.289284Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UgSOxsNklAvh",
    "outputId": "f491f83c-17b8-42ad-a225-479df8419578"
   },
   "outputs": [],
   "source": [
    "#SkipGram\n",
    "start = time.time()\n",
    "fasttext_skipgram = FastText(sentences, sg=1, min_count=10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"FastText SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:28:28.803412Z",
     "start_time": "2021-04-03T10:28:28.773386Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "vFiTAP0PsQwi",
    "outputId": "a29ae2e3-5dbc-453a-f66b-ceca255a8652"
   },
   "outputs": [],
   "source": [
    "#Summarize the loaded model\n",
    "print(fasttext_skipgram)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(fasttext_skipgram.wv.vocab)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(fasttext_skipgram['film'])}\")\n",
    "print(fasttext_skipgram['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",fasttext_skipgram.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",fasttext_skipgram.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oArMIJzYOmUR"
   },
   "source": [
    "#### An interesting obeseravtion if you noticed is that CBOW trains faster than SkipGram in both cases.\n",
    "We will leave it to the user to figure out why. A hint would be to refer the working of CBOW and skipgram."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Training_embeddings_using_gensim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:comp262_2]",
   "language": "python",
   "name": "conda-env-comp262_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
